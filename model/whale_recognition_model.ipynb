{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "34d1be7393ae2c49babbb540fc766fbc3274c579",
        "id": "xEjPsKruSDyc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Read the dataset description\n",
        "from pandas import read_csv\n",
        "\n",
        "tagged = dict([(p,w) for _,p,w in read_csv('data/train.csv').to_records()])\n",
        "submit = [p for _,p,_ in read_csv('data/sample_submission.csv').to_records()]\n",
        "join   = list(tagged.keys()) + submit\n",
        "len(tagged),len(submit),len(join),list(tagged.items())[:5],submit[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "ab6d0c498190ba97c62b96cca930a1ba2201f987",
        "id": "7XwRoL_nSDyd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Determise the size of each image\n",
        "from os.path import isfile\n",
        "from PIL import Image as pil_image\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "def expand_path(p):\n",
        "    if isfile('data/train/' + p): return 'data/train/' + p\n",
        "    if isfile('data/test/' + p): return 'data/test/' + p\n",
        "    return p\n",
        "\n",
        "p2size = {}\n",
        "for p in tqdm_notebook(join):\n",
        "    size      = pil_image.open(expand_path(p)).size\n",
        "    p2size[p] = size\n",
        "len(p2size), list(p2size.items())[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "885c2c1714e2ef38c2c30ec441684ec853d0e1a0",
        "id": "olEpvEU7SDyd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Read or generate p2h, a dictionary of image name to image id (picture to hash)\n",
        "import pickle\n",
        "import numpy as np\n",
        "from imagehash import phash\n",
        "from math import sqrt\n",
        "\n",
        "# Two phash values are considered duplicate if, for all associated image pairs:\n",
        "# 1) They have the same mode and size;\n",
        "# 2) After normalizing the pixel to zero mean and variance 1.0, the mean square error does not exceed 0.1\n",
        "def match(h1,h2):\n",
        "    for p1 in h2ps[h1]:\n",
        "        for p2 in h2ps[h2]:\n",
        "            i1 =  pil_image.open(expand_path(p1))\n",
        "            i2 =  pil_image.open(expand_path(p2))\n",
        "            if i1.mode != i2.mode or i1.size != i2.size: return False\n",
        "            a1 = np.array(i1)\n",
        "            a1 = a1 - a1.mean()\n",
        "            a1 = a1/sqrt((a1**2).mean())\n",
        "            a2 = np.array(i2)\n",
        "            a2 = a2 - a2.mean()\n",
        "            a2 = a2/sqrt((a2**2).mean())\n",
        "            a  = ((a1 - a2)**2).mean()\n",
        "            if a > 0.1: return False\n",
        "    return True\n",
        "\n",
        "if isfile('Whale/p2h.pickle'):\n",
        "    with open('Whale/p2h.pickle', 'rb') as f:\n",
        "        p2h = pickle.load(f)\n",
        "else:\n",
        "    # Compute phash for each image in the training and test set.\n",
        "    p2h = {}\n",
        "    for p in tqdm_notebook(join):\n",
        "        img    = pil_image.open(expand_path(p))\n",
        "        h      = phash(img)\n",
        "        p2h[p] = h\n",
        "\n",
        "    # Find all images associated with a given phash value.\n",
        "    h2ps = {}\n",
        "    for p,h in p2h.items():\n",
        "        if h not in h2ps: h2ps[h] = []\n",
        "        if p not in h2ps[h]: h2ps[h].append(p)\n",
        "\n",
        "    # Find all distinct phash values\n",
        "    hs = list(h2ps.keys())\n",
        "\n",
        "    # If the images are close enough, associate the two phash values (this is the slow part: n^2 algorithm)\n",
        "    h2h = {}\n",
        "    for i,h1 in enumerate(tqdm_notebook(hs)):\n",
        "        for h2 in hs[:i]:\n",
        "            if h1-h2 <= 6 and match(h1, h2):\n",
        "                s1 = str(h1)\n",
        "                s2 = str(h2)\n",
        "                if s1 < s2: s1,s2 = s2,s1\n",
        "                h2h[s1] = s2\n",
        "\n",
        "    # Group together images with equivalent phash, and replace by string format of phash (faster and more readable)\n",
        "    for p,h in p2h.items():\n",
        "        h = str(h)\n",
        "        if h in h2h: h = h2h[h]\n",
        "        p2h[p] = h\n",
        "\n",
        "len(p2h), list(p2h.items())[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "61e410583127029513732341a99145151e329475",
        "id": "S0rVHTdPSDye",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# For each image id, determine the list of pictures\n",
        "h2ps = {}\n",
        "for p,h in p2h.items():\n",
        "    if h not in h2ps: h2ps[h] = []\n",
        "    if p not in h2ps[h]: h2ps[h].append(p)\n",
        "# Notice how 25460 images use only 20913 distinct image ids.\n",
        "len(h2ps),list(h2ps.items())[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "7e768fbcafade78a656c42a1db326cdd7c24478d",
        "id": "-iK41JR9SDye",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Show an example of a duplicate image (from training of test set)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_whale(imgs, per_row=2):\n",
        "    n         = len(imgs)\n",
        "    rows      = (n + per_row - 1)//per_row\n",
        "    cols      = min(per_row, n)\n",
        "    fig, axes = plt.subplots(rows,cols, figsize=(24//per_row*cols,24//per_row*rows))\n",
        "    for ax in axes.flatten(): ax.axis('off')\n",
        "    for i,(img,ax) in enumerate(zip(imgs, axes.flatten())): ax.imshow(img.convert('RGB'))\n",
        "\n",
        "for h, ps in h2ps.items():\n",
        "    if len(ps) > 2:\n",
        "        print('Images:', ps)\n",
        "        imgs = [pil_image.open(expand_path(p)) for p in ps]\n",
        "        show_whale(imgs, per_row=len(ps))\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "x1dWI2CJSDye",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# For each images id, select the prefered image\n",
        "def prefer(ps):\n",
        "    if len(ps) == 1: return ps[0]\n",
        "    best_p = ps[0]\n",
        "    best_s = p2size[best_p]\n",
        "    for i in range(1, len(ps)):\n",
        "        p = ps[i]\n",
        "        s = p2size[p]\n",
        "        if s[0]*s[1] > best_s[0]*best_s[1]: # Select the image with highest resolution\n",
        "            best_p = p\n",
        "            best_s = s\n",
        "    return best_p\n",
        "\n",
        "h2p = {}\n",
        "for h,ps in h2ps.items(): h2p[h] = prefer(ps)\n",
        "len(h2p),list(h2p.items())[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "aec39ab5192624f08c55a330cf04804b0d63b7c1",
        "id": "kHvtKjWUSDyf"
      },
      "source": [
        "## Image rotation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "5275aba3332f8600731d19e9b9a5c3cea696e415",
        "id": "35PifWkSSDyf"
      },
      "source": [
        "I noticed that some pictures have the whale fluke pointing down instead of up as usual. Whenever I encountered such instance in the training set (not in the test set), I would add it to a list. During training, these images are rotated 180 degrees to normalize them with the fluke pointing up. The list is not exhausitve, there are probably more case that I have not noticed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "237fba51c8e460e2c362a8876ef963a37b60bbab",
        "id": "AJxCkdPTSDyf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "with open('Whale/rotate.txt', 'rt') as f: rotate = f.read().split('\\n')[:-1]\n",
        "rotate = set(rotate)\n",
        "rotate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "b4687ab632f33f5afc7d2f813a3be441927cd9ae",
        "id": "YfbLbH2dSDyf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def read_raw_image(p):\n",
        "    img = pil_image.open(expand_path(p))\n",
        "    if p in rotate: img = img.rotate(180)\n",
        "    return img\n",
        "\n",
        "p    = list(rotate)[0]\n",
        "imgs = [pil_image.open(expand_path(p)), read_raw_image(p)]\n",
        "show_whale(imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "b1b9bff9cbec6d6120abd1d11a94929481780acb",
        "id": "r41zfPeuSDyg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Read the bounding box data from the bounding box kernel (see reference above)\n",
        "with open('Whale/bounding-box.pickle', 'rb') as f:\n",
        "    p2bb = pickle.load(f)\n",
        "list(p2bb.items())[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "97c4192e165c301719289053cd8e66ce1cd2367d",
        "id": "ynx4E0SPSDyg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Suppress annoying stderr output when importing keras.\n",
        "import sys\n",
        "import platform\n",
        "old_stderr = sys.stderr\n",
        "sys.stderr = open('/dev/null' if platform.system() != 'Windows' else 'nul', 'w')\n",
        "import keras\n",
        "sys.stderr = old_stderr\n",
        "\n",
        "import random\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.image import img_to_array,array_to_img\n",
        "from scipy.ndimage import affine_transform\n",
        "\n",
        "img_shape    = (384,384,1) # The image shape used by the model\n",
        "anisotropy   = 2.15 # The horizontal compression ratio\n",
        "crop_margin  = 0.05 # The margin added around the bounding box to compensate for bounding box inaccuracy\n",
        "\n",
        "def build_transform(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
        "    \"\"\"\n",
        "    Build a transformation matrix with the specified characteristics.\n",
        "    \"\"\"\n",
        "    rotation        = np.deg2rad(rotation)\n",
        "    shear           = np.deg2rad(shear)\n",
        "    rotation_matrix = np.array([[np.cos(rotation), np.sin(rotation), 0], [-np.sin(rotation), np.cos(rotation), 0], [0, 0, 1]])\n",
        "    shift_matrix    = np.array([[1, 0, height_shift], [0, 1, width_shift], [0, 0, 1]])\n",
        "    shear_matrix    = np.array([[1, np.sin(shear), 0], [0, np.cos(shear), 0], [0, 0, 1]])\n",
        "    zoom_matrix     = np.array([[1.0/height_zoom, 0, 0], [0, 1.0/width_zoom, 0], [0, 0, 1]])\n",
        "    shift_matrix    = np.array([[1, 0, -height_shift], [0, 1, -width_shift], [0, 0, 1]])\n",
        "    return np.dot(np.dot(rotation_matrix, shear_matrix), np.dot(zoom_matrix, shift_matrix))\n",
        "\n",
        "def read_cropped_image(p, augment):\n",
        "    \"\"\"\n",
        "    @param p : the name of the picture to read\n",
        "    @param augment: True/False if data augmentation should be performed\n",
        "    @return a numpy array with the transformed image\n",
        "    \"\"\"\n",
        "    # If an image id was given, convert to filename\n",
        "    if p in h2p: p = h2p[p]\n",
        "    size_x,size_y = p2size[p]\n",
        "\n",
        "    # Determine the region of the original image we want to capture based on the bounding box.\n",
        "    x0,y0,x1,y1   = p2bb[p]\n",
        "    if p in rotate: x0, y0, x1, y1 = size_x - x1, size_y - y1, size_x - x0, size_y - y0\n",
        "    dx            = x1 - x0\n",
        "    dy            = y1 - y0\n",
        "    x0           -= dx*crop_margin\n",
        "    x1           += dx*crop_margin + 1\n",
        "    y0           -= dy*crop_margin\n",
        "    y1           += dy*crop_margin + 1\n",
        "    if (x0 < 0     ): x0 = 0\n",
        "    if (x1 > size_x): x1 = size_x\n",
        "    if (y0 < 0     ): y0 = 0\n",
        "    if (y1 > size_y): y1 = size_y\n",
        "    dx            = x1 - x0\n",
        "    dy            = y1 - y0\n",
        "    if dx > dy*anisotropy:\n",
        "        dy  = 0.5*(dx/anisotropy - dy)\n",
        "        y0 -= dy\n",
        "        y1 += dy\n",
        "    else:\n",
        "        dx  = 0.5*(dy*anisotropy - dx)\n",
        "        x0 -= dx\n",
        "        x1 += dx\n",
        "\n",
        "    # Generate the transformation matrix\n",
        "    trans = np.array([[1, 0, -0.5*img_shape[0]], [0, 1, -0.5*img_shape[1]], [0, 0, 1]])\n",
        "    trans = np.dot(np.array([[(y1 - y0)/img_shape[0], 0, 0], [0, (x1 - x0)/img_shape[1], 0], [0, 0, 1]]), trans)\n",
        "    if augment:\n",
        "        trans = np.dot(build_transform(\n",
        "            random.uniform(-5, 5),\n",
        "            random.uniform(-5, 5),\n",
        "            random.uniform(0.8, 1.0),\n",
        "            random.uniform(0.8, 1.0),\n",
        "            random.uniform(-0.05*(y1 - y0), 0.05*(y1 - y0)),\n",
        "            random.uniform(-0.05*(x1 - x0), 0.05*(x1 - x0))\n",
        "            ), trans)\n",
        "    trans = np.dot(np.array([[1, 0, 0.5*(y1 + y0)], [0, 1, 0.5*(x1 + x0)], [0, 0, 1]]), trans)\n",
        "\n",
        "    # Read the image, transform to black and white and comvert to numpy array\n",
        "    img   = read_raw_image(p).convert('L')\n",
        "    img   = img_to_array(img)\n",
        "\n",
        "    # Apply affine transformation\n",
        "    matrix = trans[:2,:2]\n",
        "    offset = trans[:2,2]\n",
        "    img    = img.reshape(img.shape[:-1])\n",
        "    img    = affine_transform(img, matrix, offset, output_shape=img_shape[:-1], order=1, mode='constant', cval=np.average(img))\n",
        "    img    = img.reshape(img_shape)\n",
        "\n",
        "    # Normalize to zero mean and unit variance\n",
        "    img  -= np.mean(img, keepdims=True)\n",
        "    img  /= np.std(img, keepdims=True) + K.epsilon()\n",
        "    return img\n",
        "\n",
        "def read_for_training(p):\n",
        "    \"\"\"\n",
        "    Read and preprocess an image with data augmentation (random transform).\n",
        "    \"\"\"\n",
        "    return read_cropped_image(p, True)\n",
        "\n",
        "def read_for_validation(p):\n",
        "    \"\"\"\n",
        "    Read and preprocess an image without data augmentation (use for testing).\n",
        "    \"\"\"\n",
        "    return read_cropped_image(p, False)\n",
        "\n",
        "p = list(tagged.keys())[312]\n",
        "imgs = [\n",
        "    read_raw_image(p),\n",
        "    array_to_img(read_for_validation(p)),\n",
        "    array_to_img(read_for_training(p))\n",
        "]\n",
        "show_whale(imgs, per_row=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "9f11d92483b771073da6c1a2f0eb62815e92aee0",
        "id": "vfUpQKueSDyh"
      },
      "source": [
        "## Code\n",
        "The following is the Keras code for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "7f90b807f6236490117f67b485bb2268b32eb8b4",
        "id": "Nc4dXE_zSDyi",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from keras import regularizers\n",
        "from keras.optimizers import Adam\n",
        "from keras.engine.topology import Input\n",
        "from keras.layers import Activation, Add, BatchNormalization, Concatenate, Conv2D, Dense, Flatten, GlobalMaxPooling2D, Lambda, MaxPooling2D, Reshape\n",
        "from keras.models import Model\n",
        "\n",
        "def subblock(x, filter, **kwargs):\n",
        "    x = BatchNormalization()(x)\n",
        "    y = x\n",
        "    y = Conv2D(filter, (1, 1), activation='relu', **kwargs)(y) # Reduce the number of features to 'filter'\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Conv2D(filter, (3, 3), activation='relu', **kwargs)(y) # Extend the feature field\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Conv2D(K.int_shape(x)[-1], (1, 1), **kwargs)(y) # no activation # Restore the number of original features\n",
        "    y = Add()([x,y]) # Add the bypass connection\n",
        "    y = Activation('relu')(y)\n",
        "    return y\n",
        "\n",
        "def build_model(lr, l2, activation='sigmoid'):\n",
        "\n",
        "    ##############\n",
        "    # BRANCH MODEL\n",
        "    ##############\n",
        "    regul  = regularizers.l2(l2)\n",
        "    optim  = Adam(lr=lr)\n",
        "    kwargs = {'padding':'same', 'kernel_regularizer':regul}\n",
        "\n",
        "    inp = Input(shape=img_shape) # 384x384x1\n",
        "    x   = Conv2D(64, (9,9), strides=2, activation='relu', **kwargs)(inp)\n",
        "\n",
        "    x   = MaxPooling2D((2, 2), strides=(2, 2))(x) # 96x96x64\n",
        "    for _ in range(2):\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Conv2D(64, (3,3), activation='relu', **kwargs)(x)\n",
        "\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2))(x) # 48x48x64\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(128, (1,1), activation='relu', **kwargs)(x) # 48x48x128\n",
        "    for _ in range(4): x = subblock(x, 64, **kwargs)\n",
        "\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2))(x) # 24x24x128\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(256, (1,1), activation='relu', **kwargs)(x) # 24x24x256\n",
        "    for _ in range(4): x = subblock(x, 64, **kwargs)\n",
        "\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2))(x) # 12x12x256\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(384, (1,1), activation='relu', **kwargs)(x) # 12x12x384\n",
        "    for _ in range(4): x = subblock(x, 96, **kwargs)\n",
        "\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2))(x) # 6x6x384\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(512, (1,1), activation='relu', **kwargs)(x) # 6x6x512\n",
        "    for _ in range(4): x = subblock(x, 128, **kwargs)\n",
        "\n",
        "    x             = GlobalMaxPooling2D()(x) # 512\n",
        "    branch_model  = Model(inp, x)\n",
        "\n",
        "    ############\n",
        "    # HEAD MODEL\n",
        "    ############\n",
        "    mid        = 32\n",
        "    xa_inp     = Input(shape=branch_model.output_shape[1:])\n",
        "    xb_inp     = Input(shape=branch_model.output_shape[1:])\n",
        "    x1         = Lambda(lambda x : x[0]*x[1])([xa_inp, xb_inp])\n",
        "    x2         = Lambda(lambda x : x[0] + x[1])([xa_inp, xb_inp])\n",
        "    x3         = Lambda(lambda x : K.abs(x[0] - x[1]))([xa_inp, xb_inp])\n",
        "    x4         = Lambda(lambda x : K.square(x))(x3)\n",
        "    x          = Concatenate()([x1, x2, x3, x4])\n",
        "    x          = Reshape((4, branch_model.output_shape[1], 1), name='reshape1')(x)\n",
        "\n",
        "    # Per feature NN with shared weight is implemented using CONV2D with appropriate stride.\n",
        "    x          = Conv2D(mid, (4, 1), activation='relu', padding='valid')(x)\n",
        "    x          = Reshape((branch_model.output_shape[1], mid, 1))(x)\n",
        "    x          = Conv2D(1, (1, mid), activation='linear', padding='valid')(x)\n",
        "    x          = Flatten(name='flatten')(x)\n",
        "\n",
        "    # Weighted sum implemented as a Dense layer.\n",
        "    x          = Dense(1, use_bias=True, activation=activation, name='weighted-average')(x)\n",
        "    head_model = Model([xa_inp, xb_inp], x, name='head')\n",
        "\n",
        "    ########################\n",
        "    # SIAMESE NEURAL NETWORK\n",
        "    ########################\n",
        "    # Complete model is constructed by calling the branch model on each input image,\n",
        "    # and then the head model on the resulting 512-vectors.\n",
        "    img_a      = Input(shape=img_shape)\n",
        "    img_b      = Input(shape=img_shape)\n",
        "    xa         = branch_model(img_a)\n",
        "    xb         = branch_model(img_b)\n",
        "    x          = head_model([xa, xb])\n",
        "    model      = Model([img_a, img_b], x)\n",
        "    model.compile(optim, loss='binary_crossentropy', metrics=['binary_crossentropy', 'acc'])\n",
        "    return model, branch_model, head_model\n",
        "\n",
        "model, branch_model, head_model = build_model(64e-5,0)\n",
        "head_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "7784a525bdebc38d2b89b106f33c2893e58ebf04",
        "id": "XX71AmYaSDyi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(head_model, to_file='head-model.png')\n",
        "pil_image.open('head-model.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "0efc2c31c8d2a1aa8aef5d135439d8cf2e5489d2",
        "id": "itQHosemSDyi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "branch_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "16cb29a5f2fe1e1923f51ec5207297dfeb69c216",
        "id": "WZpRIZ-oSDyi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plot_model(branch_model, to_file='branch-model.png')\n",
        "img = pil_image.open('branch-model.png')\n",
        "img.resize([x//2 for x in img.size])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "7e04e1c924396cda1834e675b2de8a447267116d",
        "id": "08rGVmIvSDyi"
      },
      "source": [
        "## Image selection\n",
        "To begin, we reduce the number of images from the training set:\n",
        "\n",
        "* Images from the blacklist are removed;\n",
        "* Duplicate images are removed;\n",
        "* All 'new_whale' images are removed;\n",
        "* All whales with a single image are removed.\n",
        "\n",
        "The blacklist was constructed manually by spotting images unhelpful to training. Reasons could be the underside of the fluke is not visible, or we see only dead fluke fragments on the a beach, there are two whales in the picture, etc. The list is in no way exhaustive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "a3403d1d59eb66f496f042c0a08b3286814b2117",
        "id": "WWMe-qJkSDyi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "with open('Whale/exclude.txt', 'rt') as f: exclude = f.read().split('\\n')[:-1]\n",
        "len(exclude)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "edca49e4b9e795a202c426804104d0ac917905c0",
        "id": "ZrSJVcAHSDyj",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "show_whale([read_raw_image(p) for p in exclude], per_row=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "4b0b2120d9f3d38bcc0971e0c0baab3e07dcbbcc",
        "id": "J1cHpCO-SDyj",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Find all the whales associated with an image id. It can be ambiguous as duplicate images may have different whale ids.\n",
        "h2ws = {}\n",
        "new_whale = 'new_whale'\n",
        "for p,w in tagged.items():\n",
        "    if w != new_whale: # Use only identified whales\n",
        "        h = p2h[p]\n",
        "        if h not in h2ws: h2ws[h] = []\n",
        "        if w not in h2ws[h]: h2ws[h].append(w)\n",
        "for h,ws in h2ws.items():\n",
        "    if len(ws) > 1:\n",
        "        h2ws[h] = sorted(ws)\n",
        "len(h2ws)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "8844f37d5d2c40eb9347dfb29b2383ecdf417557",
        "id": "QO78L4q5SDyj",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# For each whale, find the unambiguous images ids.\n",
        "w2hs = {}\n",
        "for h,ws in h2ws.items():\n",
        "    if len(ws) == 1: # Use only unambiguous pictures\n",
        "        if h2p[h] in exclude:\n",
        "            print(h) # Skip excluded images\n",
        "        else:\n",
        "            w = ws[0]\n",
        "            if w not in w2hs: w2hs[w] = []\n",
        "            if h not in w2hs[w]: w2hs[w].append(h)\n",
        "for w,hs in w2hs.items():\n",
        "    if len(hs) > 1:\n",
        "        w2hs[w] = sorted(hs)\n",
        "len(w2hs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "e1420dd75317af6d7876f86187713e28e4cd385d",
        "id": "9Lvb8Yc5SDyk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Find the list of training images, keep only whales with at least two images.\n",
        "train = [] # A list of training image ids\n",
        "for hs in w2hs.values():\n",
        "    if len(hs) > 1:\n",
        "        train += hs\n",
        "random.shuffle(train)\n",
        "train_set = set(train)\n",
        "\n",
        "w2ts = {} # Associate the image ids from train to each whale id.\n",
        "for w,hs in w2hs.items():\n",
        "    for h in hs:\n",
        "        if h in train_set:\n",
        "            if w not in w2ts: w2ts[w] = []\n",
        "            if h not in w2ts[w]: w2ts[w].append(h)\n",
        "for w,ts in w2ts.items(): w2ts[w] = np.array(ts)\n",
        "\n",
        "t2i = {} # The position in train of each training image id\n",
        "for i,t in enumerate(train): t2i[t] = i\n",
        "\n",
        "len(train),len(w2ts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "bce8abd37c4ccdcc0059504c03e2d4f3371fd170",
        "id": "RJTecpk_SDyk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from keras.utils import Sequence\n",
        "\n",
        "try:\n",
        "    from lap import lapjv\n",
        "    segment = False\n",
        "except ImportError:\n",
        "    print('Module lap not found, emulating with much slower scipy.optimize.linear_sum_assignment')\n",
        "    segment = True\n",
        "    from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "class TrainingData(Sequence):\n",
        "    def __init__(self, score, steps=1000, batch_size=32):\n",
        "        \"\"\"\n",
        "        @param score the cost matrix for the picture matching\n",
        "        @param steps the number of epoch we are planning with this score matrix\n",
        "        \"\"\"\n",
        "        super(TrainingData, self).__init__()\n",
        "        self.score      = -score # Maximizing the score is the same as minimuzing -score.\n",
        "        self.steps      = steps\n",
        "        self.batch_size = batch_size\n",
        "        for ts in w2ts.values():\n",
        "            idxs = [t2i[t] for t in ts]\n",
        "            for i in idxs:\n",
        "                for j in idxs:\n",
        "                    self.score[i,j] = 10000.0 # Set a large value for matching whales -- eliminates this potential pairing\n",
        "        self.on_epoch_end()\n",
        "    def __getitem__(self, index):\n",
        "        start = self.batch_size*index\n",
        "        end   = min(start + self.batch_size, len(self.match) + len(self.unmatch))\n",
        "        size  = end - start\n",
        "        assert size > 0\n",
        "        a     = np.zeros((size,) + img_shape, dtype=K.floatx())\n",
        "        b     = np.zeros((size,) + img_shape, dtype=K.floatx())\n",
        "        c     = np.zeros((size,1), dtype=K.floatx())\n",
        "        j     = start//2\n",
        "        for i in range(0, size, 2):\n",
        "            a[i,  :,:,:] = read_for_training(self.match[j][0])\n",
        "            b[i,  :,:,:] = read_for_training(self.match[j][1])\n",
        "            c[i,  0    ] = 1 # This is a match\n",
        "            a[i+1,:,:,:] = read_for_training(self.unmatch[j][0])\n",
        "            b[i+1,:,:,:] = read_for_training(self.unmatch[j][1])\n",
        "            c[i+1,0    ] = 0 # Different whales\n",
        "            j           += 1\n",
        "        return [a,b],c\n",
        "    def on_epoch_end(self):\n",
        "        if self.steps <= 0: return # Skip this on the last epoch.\n",
        "        self.steps     -= 1\n",
        "        self.match      = []\n",
        "        self.unmatch    = []\n",
        "        if segment:\n",
        "            # Using slow scipy. Make small batches.\n",
        "            # Because algorithm is O(n^3), small batches are much faster.\n",
        "            # However, this does not find the real optimum, just an approximation.\n",
        "            tmp   = []\n",
        "            batch = 512\n",
        "            for start in range(0, score.shape[0], batch):\n",
        "                end = min(score.shape[0], start + batch)\n",
        "                _, x = linear_sum_assignment(self.score[start:end, start:end])\n",
        "                tmp.append(x + start)\n",
        "            x = np.concatenate(tmp)\n",
        "        else:\n",
        "            _,_,x = lapjv(self.score) # Solve the linear assignment problem\n",
        "        y = np.arange(len(x),dtype=np.int32)\n",
        "\n",
        "        # Compute a derangement for matching whales\n",
        "        for ts in w2ts.values():\n",
        "            d = ts.copy()\n",
        "            while True:\n",
        "                random.shuffle(d)\n",
        "                if not np.any(ts == d): break\n",
        "            for ab in zip(ts,d): self.match.append(ab)\n",
        "\n",
        "        # Construct unmatched whale pairs from the LAP solution.\n",
        "        for i,j in zip(x,y):\n",
        "            if i == j:\n",
        "                print(self.score)\n",
        "                print(x)\n",
        "                print(y)\n",
        "                print(i,j)\n",
        "            assert i != j\n",
        "            self.unmatch.append((train[i],train[j]))\n",
        "\n",
        "        # Force a different choice for an eventual next epoch.\n",
        "        self.score[x,y] = 10000.0\n",
        "        self.score[y,x] = 10000.0\n",
        "        random.shuffle(self.match)\n",
        "        random.shuffle(self.unmatch)\n",
        "        # print(len(self.match), len(train), len(self.unmatch), len(train))\n",
        "        assert len(self.match) == len(train) and len(self.unmatch) == len(train)\n",
        "    def __len__(self):\n",
        "        return (len(self.match) + len(self.unmatch) + self.batch_size - 1)//self.batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "38a2392b38554486c1396ecf9614c3785c03c08c",
        "id": "DHobmoMkSDyl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Test on a batch of 32 with random costs.\n",
        "score = np.random.random_sample(size=(len(train),len(train)))\n",
        "data = TrainingData(score)\n",
        "(a, b), c = data[0]\n",
        "a.shape, b.shape, c.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "54c10e3e139a93c59a125a50329d59ef6059083b",
        "id": "manyvCNhSDyl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# First pair is for matching whale\n",
        "imgs = [array_to_img(a[0]), array_to_img(b[0])]\n",
        "show_whale(imgs, per_row=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "63da9a872cbea6641fb7a8740696b5b559fa38f9",
        "id": "k_2qDHiHSDyl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Second pair is for different whales\n",
        "imgs = [array_to_img(a[1]), array_to_img(b[1])]\n",
        "show_whale(imgs, per_row=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "d8878aaebeb16efc57a1628dc47c0a811b7c9a6e",
        "collapsed": true,
        "id": "JyTjjie_SDym",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# A Keras generator to evaluate only the BRANCH MODEL\n",
        "class FeatureGen(Sequence):\n",
        "    def __init__(self, data, batch_size=64, verbose=1):\n",
        "        super(FeatureGen, self).__init__()\n",
        "        self.data       = data\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose    = verbose\n",
        "        if self.verbose > 0: self.progress = tqdm_notebook(total=len(self), desc='Features')\n",
        "    def __getitem__(self, index):\n",
        "        start = self.batch_size*index\n",
        "        size  = min(len(self.data) - start, self.batch_size)\n",
        "        a     = np.zeros((size,) + img_shape, dtype=K.floatx())\n",
        "        for i in range(size): a[i,:,:,:] = read_for_validation(self.data[start + i])\n",
        "        if self.verbose > 0:\n",
        "            self.progress.update()\n",
        "            if self.progress.n >= len(self): self.progress.close()\n",
        "        return a\n",
        "    def __len__(self):\n",
        "        return (len(self.data) + self.batch_size - 1)//self.batch_size\n",
        "\n",
        "# A Keras generator to evaluate on the HEAD MODEL on features already pre-computed.\n",
        "# It computes only the upper triangular matrix of the cost matrix if y is None.\n",
        "class ScoreGen(Sequence):\n",
        "    def __init__(self, x, y=None, batch_size=2048, verbose=1):\n",
        "        super(ScoreGen, self).__init__()\n",
        "        self.x          = x\n",
        "        self.y          = y\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose    = verbose\n",
        "        if y is None:\n",
        "            self.y           = self.x\n",
        "            self.ix, self.iy = np.triu_indices(x.shape[0],1)\n",
        "        else:\n",
        "            self.iy, self.ix = np.indices((y.shape[0],x.shape[0]))\n",
        "            self.ix          = self.ix.reshape((self.ix.size,))\n",
        "            self.iy          = self.iy.reshape((self.iy.size,))\n",
        "        self.subbatch = (len(self.x) + self.batch_size - 1)//self.batch_size\n",
        "        if self.verbose > 0: self.progress = tqdm_notebook(total=len(self), desc='Scores')\n",
        "    def __getitem__(self, index):\n",
        "        start = index*self.batch_size\n",
        "        end   = min(start + self.batch_size, len(self.ix))\n",
        "        a     = self.y[self.iy[start:end],:]\n",
        "        b     = self.x[self.ix[start:end],:]\n",
        "        if self.verbose > 0:\n",
        "            self.progress.update()\n",
        "            if self.progress.n >= len(self): self.progress.close()\n",
        "        return [a,b]\n",
        "    def __len__(self):\n",
        "        return (len(self.ix) + self.batch_size - 1)//self.batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "f0835cea78c2d2c63ef290d5ff826f3c2a866504",
        "collapsed": true,
        "id": "5tWX22X9SDym",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from keras_tqdm import TQDMNotebookCallback\n",
        "\n",
        "def set_lr(model, lr):\n",
        "    K.set_value(model.optimizer.lr, float(lr))\n",
        "\n",
        "def get_lr(model):\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "\n",
        "def score_reshape(score, x, y=None):\n",
        "    \"\"\"\n",
        "    Tranformed the packed matrix 'score' into a square matrix.\n",
        "    @param score the packed matrix\n",
        "    @param x the first image feature tensor\n",
        "    @param y the second image feature tensor if different from x\n",
        "    @result the square matrix\n",
        "    \"\"\"\n",
        "    if y is None:\n",
        "        # When y is None, score is a packed upper triangular matrix.\n",
        "        # Unpack, and transpose to form the symmetrical lower triangular matrix.\n",
        "        m = np.zeros((x.shape[0],x.shape[0]), dtype=K.floatx())\n",
        "        m[np.triu_indices(x.shape[0],1)] = score.squeeze()\n",
        "        m += m.transpose()\n",
        "    else:\n",
        "        m        = np.zeros((y.shape[0],x.shape[0]), dtype=K.floatx())\n",
        "        iy,ix    = np.indices((y.shape[0],x.shape[0]))\n",
        "        ix       = ix.reshape((ix.size,))\n",
        "        iy       = iy.reshape((iy.size,))\n",
        "        m[iy,ix] = score.squeeze()\n",
        "    return m\n",
        "\n",
        "def compute_score(verbose=1):\n",
        "    \"\"\"\n",
        "    Compute the score matrix by scoring every pictures from the training set against every other picture O(n^2).\n",
        "    \"\"\"\n",
        "    features = branch_model.predict_generator(FeatureGen(train, verbose=verbose), max_queue_size=12, workers=6, verbose=0)\n",
        "    score    = head_model.predict_generator(ScoreGen(features, verbose=verbose), max_queue_size=12, workers=6, verbose=0)\n",
        "    score    = score_reshape(score, features)\n",
        "    return features, score\n",
        "\n",
        "def make_steps(step, ampl):\n",
        "    \"\"\"\n",
        "    Perform training epochs\n",
        "    @param step Number of epochs to perform\n",
        "    @param ampl the K, the randomized component of the score matrix.\n",
        "    \"\"\"\n",
        "    global w2ts, t2i, steps, features, score, histories\n",
        "\n",
        "    # shuffle the training pictures\n",
        "    random.shuffle(train)\n",
        "\n",
        "    # Map whale id to the list of associated training picture hash value\n",
        "    w2ts = {}\n",
        "    for w,hs in w2hs.items():\n",
        "        for h in hs:\n",
        "            if h in train_set:\n",
        "                if w not in w2ts: w2ts[w] = []\n",
        "                if h not in w2ts[w]: w2ts[w].append(h)\n",
        "    for w,ts in w2ts.items(): w2ts[w] = np.array(ts)\n",
        "\n",
        "    # Map training picture hash value to index in 'train' array\n",
        "    t2i  = {}\n",
        "    for i,t in enumerate(train): t2i[t] = i\n",
        "\n",
        "    # Compute the match score for each picture pair\n",
        "    features, score = compute_score()\n",
        "\n",
        "    # Train the model for 'step' epochs\n",
        "    history = model.fit_generator(\n",
        "        TrainingData(score + ampl*np.random.random_sample(size=score.shape), steps=step, batch_size=32),\n",
        "        initial_epoch=steps, epochs=steps + step, max_queue_size=12, workers=6, verbose=0,\n",
        "        callbacks=[\n",
        "            TQDMNotebookCallback(leave_inner=True, metric_format='{value:0.3f}')\n",
        "        ]).history\n",
        "    steps += step\n",
        "\n",
        "    # Collect history data\n",
        "    history['epochs'] = steps\n",
        "    history['ms'    ] = np.mean(score)\n",
        "    history['lr'    ] = get_lr(model)\n",
        "    print(history['epochs'],history['lr'],history['ms'])\n",
        "    histories.append(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "f84b80385ff2adbac2528fe5642f55c5bb50ea29",
        "collapsed": true,
        "id": "lB9Mrj90SDym",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model_name = 'whale-standard'\n",
        "histories  = []\n",
        "steps      = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "2563e2e6b15e7ce23f94ad5a1093ad2e70861de4",
        "collapsed": true,
        "id": "UBfkCl8CSDym",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "if isfile('../input/humpback-whale-identification-model-files/whale-standard.model'):\n",
        "    tmp = keras.models.load_model('../input/humpback-whale-identification-model-files/whale-standard.model')\n",
        "    model.set_weights(tmp.get_weights())\n",
        "else:\n",
        "    # epoch -> 10\n",
        "    make_steps(10, 1000)\n",
        "    ampl = 100.0\n",
        "    for _ in range(10):\n",
        "        print('noise ampl.  = ', ampl)\n",
        "        make_steps(5, ampl)\n",
        "        ampl = max(1.0, 100**-0.1*ampl)\n",
        "    # epoch -> 150\n",
        "    for _ in range(18): make_steps(5, 1.0)\n",
        "    # epoch -> 200\n",
        "    set_lr(model, 16e-5)\n",
        "    for _ in range(10): make_steps(5, 0.5)\n",
        "    # epoch -> 240\n",
        "    set_lr(model, 4e-5)\n",
        "    for _ in range(8): make_steps(5, 0.25)\n",
        "    # epoch -> 250\n",
        "    set_lr(model, 1e-5)\n",
        "    for _ in range(2): make_steps(5, 0.25)\n",
        "    # epoch -> 300\n",
        "    weights = model.get_weights()\n",
        "    model, branch_model, head_model = build_model(64e-5,0.0002)\n",
        "    model.set_weights(weights)\n",
        "    for _ in range(10): make_steps(5, 1.0)\n",
        "    # epoch -> 350\n",
        "    set_lr(model, 16e-5)\n",
        "    for _ in range(10): make_steps(5, 0.5)\n",
        "    # epoch -> 390\n",
        "    set_lr(model, 4e-5)\n",
        "    for _ in range(8): make_steps(5, 0.25)\n",
        "    # epoch -> 400\n",
        "    set_lr(model, 1e-5)\n",
        "    for _ in range(2): make_steps(5, 0.25)\n",
        "    model.save('whale-standard.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "a02a1fa6e97259186174afc82a4599e03d047d2b",
        "id": "dU6drpsYSDyn"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "eb8f53368668723f327040bb3b058f005d38ba77",
        "id": "Ju0fSLsISDyn"
      },
      "source": [
        "# Bootstrapping and ensemble\n",
        "The whale-standard.model is good for a 0.766 score.\n",
        "\n",
        "Because the training data set is small, and the test set is larger, bootstrapping is a good candidate to improve the score. In this context, bootstrapping means using the model to automatically generate additional training example, and retrained the model over this larger dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b832dd2d0d837eea499b129184f6633e193ade48",
        "id": "WeVVAwy5SDyn"
      },
      "source": [
        "## Bootstrapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "fcef7ef0b197305f6944e99346451cd3bca63fce",
        "id": "E_zv2NrZSDyn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "with open('Whale/bootstrap.pickle', 'rb') as f:\n",
        "    bootstrap = pickle.load(f)\n",
        "len(bootstrap), list(bootstrap.items())[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "42062beebd3413eeced9706ddf629406dba773f3",
        "id": "fOobyK7gSDyn"
      },
      "source": [
        "Submitting these 1885 pictures as a submission show that this set if over 93% accurate.\n",
        "\n",
        "Adding these files to the training set and re-running the training from scratch generates the whale.bootstrap.model, also included in the dataset. This model as a slightly better score of 0.744 with a threshold=0.989."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "664531df0a9fd9cf7f59d02b47cf0a612a3d9522",
        "id": "4oj3YVKOSDyo"
      },
      "source": [
        "## Ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f294a99f12859710df82c9c6f663d5d0d7f3aaa9",
        "id": "-CrIir99SDyo"
      },
      "source": [
        "The best score is obtained by an ensemble of the whale-standard.model and whale-bootstrap.model. Both of these models make different errors because of their nature, which make them good candidate for ensembling:\n",
        "\n",
        "* The standard model is trained on the smallest training set, and thus has more potential for overfitting.\n",
        "* The bootstrap model is trained on more data, however the tagging accuracy is lower since the bootstrap data is only 93% accurate.\n",
        "\n",
        "The assembly strategy consist in compute a score matrix (or dimension test by train) that is a linear combination of the standard and bootstrap model. Generation of the submission using the score matrix is unchanged. Trial and error suggest a weight of 0.45 for the standard model and 0.55 for the bootstrap model.\n",
        "\n",
        "The resulting ensemble as an accuracy of **0.78563** using a threshold of 0.92. It is interesting to note how the 'threshold' value for the ensemble is much lower, which is consistent with the fact that both models make different errors, and thus the ensemble scores are typically lower than the individual models, which are grossly optimistic about they guesses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "69020ecc39f36ad16c670da837e6c336580525e8",
        "id": "sU3NMhzZSDyo"
      },
      "source": [
        "# Visualization\n",
        "This section explores the model through some visualizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "ca8713813b6e8d8fff5ce8c031f4d2cd05f48561",
        "id": "6QIkO3JlSDyo"
      },
      "source": [
        "## Feature weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "3ec585afe83f6fa115c2867c8ff60f0f3f066b0a",
        "id": "93rjr9CrSDyo"
      },
      "source": [
        "As was discussed in the model description, the head model makes a weighted sum of the features, allowing for negative weights. We can verify that we see a combination of positive and negative weights, that confirm that some features, when matched, reduce the probability of matching whales. This could be that we match uniform, unicolor flukes, which is less likely to be correct that mathing flukes with multiple caracteristic markings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "a67b17eb5b0472789036dff08b064cb709d13389",
        "id": "L3Zwdi7JSDyo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "w = head_model.layers[-1].get_weights()[0]\n",
        "w = w.flatten().tolist()\n",
        "w = sorted(w)\n",
        "fig, axes = plt.subplots(1,1)\n",
        "axes.bar(range(len(w)), w)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "7dee1a76306cba30215867133189f58f56a6c90e",
        "id": "6Ye7kyqhSDyo"
      },
      "source": [
        "We can also check how 'per feature' network behave for different feature values.\n",
        "\n",
        "What we expect to see is that equal zero feature should produce a smaller output than similar large values. At the same time, very dissimilar values must be penalized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "2af9ec4e2c5df9faa21aba4ed37833486ade6b55",
        "collapsed": true,
        "id": "HUkXuStHSDyo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Construct the head model with linear activation\n",
        "_, _, tmp_model = build_model(64e-5,0, activation='linear')\n",
        "tmp_model.set_weights(head_model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "e937b7a76cc7e2b6e7474592a993e9d837437c03",
        "id": "IUCzGftiSDyp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Evaluate the model for constant vectors.\n",
        "a = np.ones((21*21,512),dtype=K.floatx())\n",
        "b = np.ones((21*21,512),dtype=K.floatx())\n",
        "for i in range(21):\n",
        "    for j in range(21):\n",
        "        a[21*i + j] *= float(i)/10.0\n",
        "        b[21*i + j] *= float(j)/10.0\n",
        "x    = np.arange(0.0, 2.01, 0.1, dtype=K.floatx())\n",
        "x, y = np.meshgrid(x, x)\n",
        "z    = tmp_model.predict([a,b], verbose=0).reshape((21,21))\n",
        "x.shape, y.shape, z.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "0010e75e700bfa3501d7ff8bf73c7be764ad62e0",
        "id": "5mDeRqmsSDyp"
      },
      "source": [
        "## Pseudo-distance function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "ab38b8c09129211f8d79cfd569ba4740cb56e540",
        "id": "bCZW4swJSDyp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(x, y, z, cmap=cm.coolwarm)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "1b76640bfb5f4c466a8077d38fc928246c8949df",
        "id": "tp_OOXgfSDyp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import BoundaryNorm\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "levels = MaxNLocator(nbins=15).tick_values(z.min(), z.max())\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cf = ax.contourf(x, y, z, levels=levels, cmap=cm.coolwarm)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "ced4199e3979bc63c12949047ca88af4fb9c1538",
        "id": "WnyCvRJ4SDyp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "def show_filter(filter, blur):\n",
        "    np.random.seed(1)\n",
        "    noise   = 0.1 # Initial noise\n",
        "    step    = 1 # Gradient step\n",
        "\n",
        "    # Construct the function\n",
        "    inp     = branch_model.layers[0].get_input_at(0)\n",
        "    loss    = K.mean(branch_model.layers[-3].output[0,2:4,2:4,filter]) # Stimulate the 4 central cells\n",
        "    grads   = K.gradients(loss, inp)[0]\n",
        "    grads  /= K.sqrt(K.mean(K.square(grads))) + K.epsilon()\n",
        "    iterate = K.function([inp],[grads])\n",
        "    img     = (np.random.random(img_shape) -0.5)*noise\n",
        "    img     = np.expand_dims(img, 0)\n",
        "\n",
        "    # Use gradient descent to form image\n",
        "    for i in range(200):\n",
        "        grads_value = iterate([img])[0]\n",
        "        # Blurring a little creates nicer images by reducing reconstruction noise\n",
        "        img = gaussian_filter(img + grads_value*step, sigma=blur)\n",
        "\n",
        "    # Clip the image to improve contrast\n",
        "    avg  = np.mean(img)\n",
        "    std  = sqrt(np.mean((img - avg)**2))\n",
        "    low  = avg - 5*std\n",
        "    high = avg + 5*std\n",
        "    return array_to_img(np.minimum(high, np.maximum(low, img))[0])\n",
        "\n",
        "# Show the first 25 features (of 512)\n",
        "show_whale([show_filter(i, 0.5) for i in tqdm_notebook(range(25))], per_row=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "ac0cbe07b888d80c4ffb7121bfd9a4f658816b65",
        "id": "gvzcPgfQSDyq"
      },
      "source": [
        "## Interesting results and scores\n",
        "Score | Description\n",
        "---:|-------\n",
        "0.786 | best score obtained by a linear combination of the standard model with the bootstrap model\n",
        "0.774 | bootstrapped model\n",
        "0.766 | standard model\n",
        "0.752 | VGG like CNN, trained like standard model\n",
        "0.728 | standard model without L2 regularization (result after 250 epochs)\n",
        "0.714 | standard model without exclusion list, rotation list and bounding box model (i.e. no manual judgement on the training set)\n",
        "0.423 | duplicate images and new_whale submission\n",
        "0.325 | new_whale submission\n",
        "0.107 | duplicate images only\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
